\documentclass[a4paper, 11pt]{article}

% --- PAQUETES ---
\usepackage[utf8]{inputenc}
\usepackage[spanish]{babel}
\usepackage{geometry}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{tikz}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{float}

% --- CONFIGURACIÓN DE PAQUETES ---

% Geometría de la página
\geometry{a4paper, margin=2.5cm}

% Configuración de TikZ
\usetikzlibrary{arrows.meta, positioning, shapes.geometric, fit, backgrounds, shapes.misc, shapes, calc}

\tikzset{
    server/.style={draw, rectangle, rounded corners, minimum height=1cm, minimum width=2.5cm},
    component/.style={draw, ellipse, minimum height=1cm},
    database/.style={draw, cylinder, shape border rotate=90, aspect=0.25, minimum height=1.2cm, minimum width=1.2cm},
    flow/.style={->, >=Stealth, thick},
    line/.style={-, thick}
}

% Colores para el documento
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,      
    urlcolor=cyan,
}
\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}

% Configuración de Listings para bloques de código
\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=2
}
\definecolor{darkgray}{rgb}{0.3,0.3,0.3}
\definecolor{darkgreen}{rgb}{0,0.5,0}
\lstdefinelanguage{yaml}{
  keywords={true,false,null},
  keywordstyle=\color{darkgray}\bfseries,
  basicstyle=\ttfamily\small,
  comment=[l]{\#},
  commentstyle=\color{purple}\ttfamily,
  stringstyle=\color{darkgreen},
  showstringspaces=false,
  morestring=[b]',
  morestring=[b]",
  alsoletter={:},
  alsodigit={-}
}
\lstset{style=mystyle}


% --- TÍTULO ---
\title{Documentación de Arquitectura: Clúster Kubernetes Híbrido}
\author{By Luis Antonio Calvo Quispe}
\date{\today}

% --- DOCUMENTO ---
\begin{document}
\sloppy

\maketitle
\tableofcontents
\newpage

\section{Resumen General}
Este documento detalla la arquitectura de un clúster de Kubernetes auto-alojado (on-premise) diseñado para alta disponibilidad, flexibilidad y exposición segura de servicios a Internet. La arquitectura se caracteriza por su naturaleza híbrida, combinando nodos de diferentes arquitecturas (x86\_64 y ARM64), un sistema de almacenamiento centralizado basado en NFS, y un doble esquema de balanceo de carga para el plano de control y los servicios.

A continuación, se presenta un resumen de los pilares fundamentales de esta arquitectura:

\begin{itemize}
    \item \textbf{Arquitectura de Nodos Híbrida:} El clúster se compone de una mezcla de nodos con arquitecturas \texttt{x86\_64} (basados en Intel N100 y AMD 5825u) y \texttt{aarch64} (Raspberry Pi). Esto permite optimizar las cargas de trabajo, utilizando nodos potentes para aplicaciones exigentes y nodos de bajo consumo para tareas más ligeras.

    \item \textbf{Plano de Control en Alta Disponibilidad:} El plano de control se ejecuta en tres nodos maestros dedicados (\texttt{x86\_64}). Para garantizar un acceso robusto al API Server de Kubernetes, se implementa una solución externa de alta disponibilidad mediante \textbf{Keepalived} y \textbf{HAProxy}. Esta configuración proporciona una IP Virtual (VIP) única (\texttt{192.168.100.230}) para todo el tráfico administrativo, haciendo que el clúster sea resiliente a fallos en los nodos maestros.

    \item \textbf{Balanceo de Carga de Doble Capa:} La arquitectura de red cuenta con dos sistemas de balanceo de carga distintos:
    \begin{enumerate}
        \item \textbf{Plano de Control:} La pila Keepalived/HAProxy para la gestión interna del clúster.
        \item \textbf{Servicios:} \textbf{MetalLB} se utiliza para exponer servicios a la red local, asignándoles IPs externas de un rango dedicado (\texttt{192.168.100.240-192.168.100.254}). El \textbf{Nginx Ingress Controller}, que gestiona todo el tráfico HTTP/S, es expuesto a través de MetalLB en la IP \texttt{192.168.100.240}.
    \end{enumerate}

    \item \textbf{Exposición Segura a Internet:} Los servicios se exponen a Internet mediante un túnel de \textbf{Fast Reverse Proxy (FRP)}. Un cliente (\texttt{frpc}) que se ejecuta dentro del clúster se conecta a un servidor público (\texttt{frps}) en un VPS, reenviando de forma segura el tráfico desde Internet al Nginx Ingress Controller.

    \item \textbf{Sistema de Almacenamiento por Niveles:} Una arquitectura de almacenamiento de múltiples niveles proporciona flexibilidad para las diferentes necesidades de las aplicaciones:
    \begin{itemize}
        \item \textbf{Nivel Rápido:} Almacenamiento SSD de alto rendimiento a través de NFS para bases de datos y aplicaciones críticas.
        \item \textbf{Nivel Masivo:} Almacenamiento HDD de alta capacidad a través de NFS para copias de seguridad y archivos.
        \item \textbf{Nivel Distribuido (Planificado):} Una futura solución CSI basada en Ceph para proporcionar volúmenes \texttt{ReadWriteMany} para bases de datos distribuidas y almacenamiento compartido.
    \end{itemize}

    \item \textbf{Gestión Automatizada de TLS:} La gestión de certificados TLS está totalmente automatizada mediante \textbf{Cert-Manager}, que se integra con \textbf{Let's Encrypt} para aprovisionar y renovar certificados SSL para todos los servicios expuestos a Internet.
\end{itemize}

Este diseño da como resultado una plataforma resiliente, flexible y segura para desplegar y gestionar aplicaciones en contenedores.

\newpage

\section{Arquitectura de Red}
La red es un pilar fundamental de este clúster, dividida en dos sistemas de balanceo de carga completamente independientes, cada uno con un propósito distinto:

\begin{itemize}
    \item \textbf{Balanceo del Plano de Control (Keepalived + HAProxy):} Este sistema se encarga exclusivamente de garantizar la alta disponibilidad del \textbf{API Server de Kubernetes}. Funciona a nivel de TCP (Capa 4) y opera de forma externa a la lógica de Kubernetes. Es esencial tener un punto de acceso único y robusto para que los nodos y los administradores puedan comunicarse con el clúster en todo momento. No se puede usar un balanceador interno de Kubernetes para esta tarea, ya que estos dependen de que el API Server ya esté funcionando.
    
    \medskip
    \noindent\fbox{\parbox{\dimexpr\linewidth-2\fboxsep-2\fboxrule\relax}{ 
    \textbf{Analisis de Impacto:}    
    Se perdería la capacidad de \textbf{administrar el clúster} (los comandos \texttt{kubectl} fallarían) y se detendrían las tareas de auto-reparación. Sin embargo, es crucial entender que las \textbf{aplicaciones en ejecución no se verían afectadas} a corto plazo y seguirían atendiendo tráfico, ya que el plano de datos es independiente.
    }}
    \medskip

    \item \textbf{Balanceo de Servicios (MetalLB):} Este sistema está integrado con Kubernetes y su función es exponer los \textbf{servicios y aplicaciones} que se ejecutan dentro del clúster a la red local. Cuando se crea un servicio de tipo \texttt{LoadBalancer}, MetalLB le asigna una IP externa de su propio rango. Este balanceador gestiona el tráfico de las aplicaciones, no el tráfico administrativo del clúster.

    \medskip
    \noindent\fbox{\parbox{\dimexpr\linewidth-2\fboxsep-2\fboxrule\relax}{
    \textbf{Analisis de Impacto:}
    Un fallo en MetalLB impediría que se anuncien las IPs de los servicios en la red. Esto provocaría la \textbf{pérdida de acceso externo a todas las aplicaciones} a través del Ingress Controller, ya que su IP se volvería inalcanzable. A diferencia del caso anterior, la \textbf{administración del clúster (\texttt{kubectl}) seguiría funcionando} con normalidad, permitiendo diagnosticar y resolver el problema sin afectar la gestión del sistema.
    }}
    \medskip
    
    \item \textbf{Exposición a Internet (FRP):} Dado que el clúster es privado, se utiliza un cliente \textbf{FRP (Fast Reverse Proxy)} para exponer los servicios a Internet. Este componente establece un túnel reverso con un servidor FRP en un VPS público y redirecciona el tráfico externo (ej. puertos 80 y 443) desde el VPS hacia la IP del Ingress Controller (\texttt{192.168.100.240}), haciendo accesibles las aplicaciones desde fuera de la red local.
    
    \medskip
    \noindent\fbox{\parbox{\dimexpr\linewidth-2\fboxsep-2\fboxrule\relax}{
    \textbf{Analisis de Impacto:}
    Un fallo en el sistema FRP (ya sea en el cliente o en el servidor) cortaría el túnel reverso. Esto impediría únicamente el \textbf{acceso a las aplicaciones desde Internet}. El clúster seguiría funcionando y siendo accesible tanto para la \textbf{administración} como para los \textbf{usuarios de la red local}.
    }}
    \medskip

\end{itemize}
Esta separación es fundamental: uno asegura la estabilidad y el acceso al cerebro del clúster (el plano de control), mientras que el otro se encarga de dar acceso a las aplicaciones que este orquesta.

\begin{table}[h!]
\centering
\begin{tabular}{|l|l|l|}
\hline
\textbf{IP} & \textbf{Componente Principal} & \textbf{Función} \\
\hline
192.168.100.230 & Keepalived + HAProxy & VIP para el plano de control de Kubernetes (API Server). \\
\hline
192.168.100.240 & MetalLB + Nginx Ingress & IP externa para servicios expuestos (Ingress). \\
\hline
\end{tabular}
\caption{Tabla Comparativa de IPs de Balanceo de Carga.}
\label{tab:ip_comparison}
\end{table}

A continuación, se detallan los componentes de cada sistema.

\subsection{Plano de Control y Alta Disponibilidad}
Para garantizar la disponibilidad del API Server de Kubernetes, se utiliza una combinación de \textbf{Keepalived} y \textbf{HAProxy}.
\begin{itemize}
    \item \textbf{Keepalived:} Gestiona una Dirección IP Virtual (VIP) \textbf{192.168.100.230}. Esta IP actúa como el punto de entrada flotante para el plano de control. El nodo Maestro del balanceador es \texttt{n100-004}, con los servidores NAS \texttt{(nas-001,nas-002,...)} actuando como respaldo.
    \item \textbf{HAProxy:} Se ejecuta en los nodos del balanceador y distribuye el tráfico TCP del puerto 6443 (API de Kubernetes) entre los tres nodos Maestros del clúster: \texttt{n100-001}, \texttt{n100-002} y \texttt{n100-003}.
    \item \textbf{Estadísticas de HAProxy:} El estado del balanceador y sus backends se puede consultar en \url{http://161.132.4.98:8404/stats}
\end{itemize}
\begin{figure}[H]
\centering
\begin{tikzpicture}[node distance=1.2cm, auto]
    % Nodos
    \node[component] (vip) {VIP: 192.168.100.230};
    \node[component, below=1.0cm of vip] (keepalived) {Keepalived};
    
    \node[server, align=center, below left=1.0cm and 2.5cm of keepalived] (haproxy_primary) {HAProxy (Primary)\\\small(n100-004)\\\tiny 192.168.100.184};
    \node[server, align=center, below right=1.0cm and 2.5cm of keepalived] (haproxy_backup) {HAProxy (Backups)\\\small(nas-001, ...)\\\tiny 192.168.100.171, ...};
    
    \node[server, align=center, below left=2cm and 0.5cm of haproxy_primary] (master1) {Master 1\\\small(n100-001)\\\tiny 192.168.100.181};
    \node[server, align=center, below=2cm of haproxy_primary] (master2) {Master 2\\\small(n100-002)\\\tiny 192.168.100.182};
    \node[server, align=center, below right=2cm and 0.5cm of haproxy_primary] (master3) {Master 3\\\small(n100-003)\\\tiny 192.168.100.183};

    % Conexiones
    \draw[flow] (vip) -- (keepalived);
    \draw[flow] (keepalived) -- (haproxy_primary);
    \draw[flow, dashed] (keepalived) -- (haproxy_backup);
    
    \draw[flow] (haproxy_primary) -- (master1);
    \draw[flow] (haproxy_primary) -- (master2);
    \draw[flow] (haproxy_primary) -- (master3);
    
    \draw[flow, dashed] (haproxy_backup) -- (master1);
    \draw[flow, dashed] (haproxy_backup) -- (master2);
    \draw[flow, dashed] (haproxy_backup) -- (master3);
    
    % Anotacion de Stats
    \node[draw, rectangle, rounded corners, fill=yellow!20, text width=3.5cm, align=center, above right=-0.5cm and 2.5cm of haproxy_backup] (stats) {HAProxy Stats:\\ \url{http://192.168.100.230:8404/stats}};
\end{tikzpicture}
\caption{Diagrama del Plano de Control con Alta Disponibilidad.} % Update caption
\label{fig:control_plane_diagram}
\end{figure}
Al inicializar el clúster con \texttt{kubeadm}, se especifica esta VIP como el endpoint del plano de control:
\begin{lstlisting}[language=bash]
\end{lstlisting}

\subsection{Exposición de Servicios y Tráfico de Entrada (Ingress)}
La exposición de servicios a la red local y a Internet se gestiona de la siguiente manera:
\begin{itemize}
    \item \textbf{MetalLB:} Actúa como balanceador de carga de red para el clúster. Proporciona direcciones IP externas a servicios de tipo \texttt{LoadBalancer}. Está configurado en modo Layer 2 para anunciar IPs desde el rango \textbf{192.168.100.240-192.168.100.254}.
    \item \textbf{Nginx Ingress Controller:} Es el principal punto de entrada para el tráfico HTTP/S. Su servicio se expone a la red local mediante MetalLB, que le asigna la IP \textbf{192.168.100.240}.
\end{itemize}
\begin{figure}[H]
\centering
\begin{tikzpicture}[node distance=1.2cm, auto]
    % Nodos
    \node[server] (metallb) {MetalLB};
    \node[component, align=center, below=0.6 cm of metallb] (ingress) {Nginx Ingress \\ \small192.168.100.240};
    
    % Adjusting distances for better spread
    \node[server, below left=1.0cm and 3.5cm of ingress, align=center] (smartshell) {SmartShell \\ \small(raspberry-002)};
    \node[server, below left=1.0cm and -0.5cm of ingress, align=center] (gitlab) {GitLab \\ \small(5825u-002)};
    \node[server, below right=1.0cm and -0.5cm of ingress, align=center] (harbor) {Harbor \\ \small(5825u-002)};
    \node[server, below right=1.0cm and 3.5cm of ingress, align=center] (apps) {...}; 

    % Conexiones
    \draw[flow] (metallb) -- node[midway, right] {\tiny(Asigna IP 192.168.100.240-254)} (ingress);
    \draw[flow] (ingress) -- node[midway, above] {\tiny(Rutas HTTP/S)} (smartshell);
    \draw[flow] (ingress) -- node[midway, above] {\tiny(Rutas HTTP/S)} (gitlab);
    \draw[flow] (ingress) -- node[midway, above] {\tiny(Rutas HTTP/S)} (harbor);
    \draw[flow] (ingress) -- node[midway, above] {\tiny(Rutas HTTP/S)} (apps); % Connection from ingress to apps
    \draw[dotted, ultra thick] (harbor) -- (apps); % Connection from harbor to apps
\end{tikzpicture}
\caption{Diagrama de Ingress y MetalLB.}
\label{fig:ingress_metallb_diagram}
\end{figure}

\subsection{FRP (Fast Reverse Proxy)}
FRP sirve para la exposición de servicios a Internet. Funciona mediante un par de aplicaciones: un servidor (\texttt{frps}) en una máquina pública y un cliente (\texttt{frpc}) dentro de la red local.

\subsubsection{Servidor FRP (frps)}
El servidor FRP se ejecuta en un VPS con una IP pública estática (\texttt{161.132.4.98}).
\begin{itemize}
    \item \textbf{Función:} Actúa como punto de encuentro para los clientes FRP, recibiendo conexiones de ellos y reenviando el tráfico público entrante a través de los túneles establecidos.
    \item \textbf{Panel de Control:} Ofrece un panel de control web para monitorear el estado de los clientes y los túneles, accesible en \url{http://161.132.4.98:7500/}.
\end{itemize}

\subsubsection{Cliente FRP (frpc)}
El cliente FRP se despliega dentro del clúster de Kubernetes.
\begin{itemize}
    \item \textbf{Función:} Establece una conexión persistente (túnel) con el servidor \texttt{frps}.
    \item \textbf{Redirección de Tráfico:} Su configuración principal consiste en reenviar el tráfico recibido en los puertos públicos 80 y 443 del servidor \texttt{frps} hacia la IP del Ingress Controller de Nginx dentro del clúster (\texttt{192.168.100.240}). De esta manera, las peticiones que llegan a la IP pública son dirigidas de forma segura a las aplicaciones que se ejecutan en Kubernetes.
\end{itemize}

\begin{figure}[H]
\centering
\begin{tikzpicture}[node distance=1.2cm, auto]
    % Nodos
    \node[component] (internet) {Internet};{GitLab}
    \node[server, below=0.5cm of internet, align=center] (vps) {FRPS (Servidor) \\ \tiny161.132.4.98};
    \node[server, below=1.0cm of vps, align=center] (frpc) {FRPC (Cliente) \\ \small(En Clúster K8s)};
    \node[component, below=0.5cm of frpc, align=center] (ingress_ip) {IP de Ingress \\ \tiny192.168.100.240};

    % Conexiones
    \draw[flow] (internet) -- (vps);
    \draw[flow] (vps) -- node[midway, right] {\tiny(Túnel Reverso)} (frpc);
    \draw[flow] (frpc) -- (ingress_ip);
\end{tikzpicture}
\caption{Diagrama del Túnel FRP.}
\label{fig:frp_tunnel_diagram}
\end{figure}

\section{Arquitectura y Flujo General del Tráfico de Red}
La arquitectura del clúster se basa en dos sistemas de balanceo de carga independientes que atienden a dos propósitos distintos: uno para la **gestión del clúster** (plano de control) y otro para la **exposición de aplicaciones** (plano de datos). El siguiente diagrama unifica los flujos de tráfico para proporcionar una visión general, integrando los componentes de red en una sola vista.

\begin{figure}[H]
\centering
\begin{tikzpicture}[node distance=1.2cm, auto]
    % Nodos
    \node[component] (internet) {Internet};{GitLab}
    \node[server, below=0.5cm of internet, align=center] (vps) {FRPS (Servidor) \\ \tiny161.132.4.98};
    \node[server, below=1.0cm of vps, align=center] (frpc) {FRPC (Cliente) \\ \small(En Clúster K8s)};
    \node[component, below=0.5cm of frpc, align=center] (ingress) {IP de Ingress \\ \tiny192.168.100.240};
    \node[server, below left=0.75cm and 2.5cm of frpc, align=center] (metallb) {MetalLB \\ \tiny192.168.100.240-254};
    \node[server, below left=1.5cm and 3.5cm of ingress, align=center] (smartshell) {SmartShell \\ \small(raspberry-002) \\ \tiny192.168.100.102};
    \node[server, below left=1.5cm and -0.5cm of ingress, align=center] (gitlab) {GitLab \\ \small(5825u-002) \\ \tiny192.168.100.152};
    \node[server, below right=1.5cm and -0.5cm of ingress, align=center] (harbor) {Harbor \\ \small(5825u-002) \\ \tiny192.168.100.152};
    \node[server, below right=1.5cm and 3.5cm of ingress, align=center] (apps) {... \\ \small(others) \\ \tiny192.168.100.*}; 

    % Conexiones
    \draw[flow] (internet) -- (vps);
    \draw[flow] (vps) -- node[midway, right] {\tiny(Túnel Reverso)} (frpc);
    \draw[flow] (frpc) -- (ingress);
    \draw[flow] (metallb) -- node[midway, above] {\tiny(Asigna IP)} (ingress);
    \draw[flow] (ingress) -- node[midway, above] {\tiny(Rutas HTTP/S)} (smartshell);
    \draw[flow] (ingress) -- node[midway, above] {\tiny(Rutas HTTP/S)} (gitlab);
    \draw[flow] (ingress) -- node[midway, above] {\tiny(Rutas HTTP/S)} (harbor);
    \draw[flow] (ingress) -- node[midway, above] {\tiny(Rutas HTTP/S)} (apps); % Connection from ingress to apps
    \draw[dotted, ultra thick] (harbor) -- (apps); % Connection from harbor to apps
\end{tikzpicture}
\caption{Diagrama Integrado de Flujo de Tráfico y Balanceo de Carga.}
\label{fig:integrated_flow_diagram}
\end{figure}

\section{Nodos del Clúster}
El clúster es heterogéneo, compuesto por nodos de diferentes arquitecturas para optimizar el consumo de energía y el rendimiento según la carga de trabajo.

\subsection{Nodos Maestros (Control Plane)}
El plano de control reside en tres nodos idénticos para garantizar quórum y alta disponibilidad.
\begin{itemize}
    \item \textbf{Hosts:} \texttt{n100-001}, \texttt{n100-002}, \texttt{n100-003}.
    \item \textbf{Arquitectura:} x86\_64 (basados en CPUs Intel N100).
    \item \textbf{Rol:} Ejecutan los componentes críticos de Kubernetes como \texttt{etcd}, \texttt{kube-apiserver}, \texttt{kube-scheduler} y \texttt{kube-controller-manager}.
\end{itemize}

\subsection{Nodos de Trabajo (Workers)}
Los nodos de trabajo son una mezcla de arquitecturas ARM64 y x86\_64 para optimizar las cargas de trabajo.
\begin{itemize}
    \item \textbf{Hosts ARM64:} Múltiples Raspberry Pi (\texttt{raspberry-001} a \texttt{raspberry-008}). Ideales para cargas de trabajo ligeras y de bajo consumo. Su arquitectura es \texttt{aarch64}.
    \item \textbf{Hosts x86\_64:} Nodos adicionales (\texttt{5825u-001}, etc.) para cargas de trabajo que requieren la arquitectura amd64.
\end{itemize}
El uso de arquitecturas mixtas requiere la creación de imágenes de contenedor multi-arquitectura para asegurar que las aplicaciones puedan ser desplegadas en cualquier nodo.

\begin{table}[H]
\centering
\begin{tabular}{|l|l|l|l|l|}
\hline
\textbf{NAME} & \textbf{STATUS} & \textbf{ROLES} & \textbf{AGE} & \textbf{VERSION} \\
\hline
5825u-001     & Ready         & worker        & 27d & v1.32.3 \\
5825u-002     & Ready         & worker        & 27d & v1.32.3 \\
n100-001      & Ready         & control-plane & 98d & v1.32.3 \\
n100-002      & Ready         & control-plane & 98d & v1.32.3 \\
n100-003      & Ready         & control-plane & 98d & v1.32.3 \\
raspberry-001 & Ready         & worker        & 98d & v1.32.3 \\
raspberry-002 & Ready         & worker        & 98d & v1.32.3 \\
raspberry-003 & Ready         & worker        & 98d & v1.32.3 \\
raspberry-004 & Ready         & worker        & 98d & v1.32.3 \\
raspberry-005 & Ready         & worker        & 98d & v1.32.3 \\
raspberry-006   & Ready         & worker          & 98d   & v1.32.3 \\
raspberry-007   & Ready         & worker          & 98d   & v1.32.3 \\
raspberry-008   & Ready         & worker          & 98d   & v1.32.3 \\
\hline
\end{tabular}
\caption{Estado de los Nodos del Clúster.}
\end{table}

\section{Arquitectura de Almacenamiento}
El almacenamiento persistente del clúster se basa en una arquitectura multi-capa (multi-tiered) que utiliza diferentes tecnologías para satisfacer distintas necesidades de rendimiento, capacidad y políticas de acceso. Todas las soluciones se integran a través de la interfaz CSI (Container Storage Interface) de Kubernetes.

\begin{itemize}
    \item \textbf{Almacenamiento Rápido (SSD):} Compuesto por los servidores \texttt{nas-001} a \texttt{nas-003}. Cada uno ofrece 2TB de almacenamiento SSD a través de NFS. Está diseñado para cargas de trabajo que requieren alto rendimiento de I/O, como bases de datos, cachés o aplicaciones críticas. La política de acceso para estas \texttt{StorageClass} es \texttt{ReadWriteOnce}.

    \item \textbf{Almacenamiento Masivo (Glacier):} Provisto por los nodos \texttt{raspberry-009} y \texttt{raspberry-010}. Este nivel ofrece una gran capacidad de 36TB sobre discos duros en RAID 5. Su rendimiento es más bajo, por lo que es ideal para backups, archivado de logs y datos de acceso poco frecuente. La política de acceso también es \texttt{ReadWriteOnce}.

    \item \textbf{Almacenamiento Distribuido (Ceph/CSI):} \textit{(En implementación)} Futura solución de almacenamiento basada en un clúster CSI (probablemente Ceph) para ofrecer la máxima eficiencia y redundancia. Soportará la política \texttt{ReadWriteMany}, permitiendo que múltiples pods escriban en el mismo volumen simultáneamente. Es la solución elegida para bases de datos en alta disponibilidad y volúmenes compartidos. La capacidad inicial será de 500GB en los nodos \texttt{010-013}.
\end{itemize}

A continuación, se presenta una tabla con el inventario de los dispositivos de almacenamiento:

\begin{table}[H]
\centering
\begin{tabular}{|l|l|l|l|l|}
\hline
\textbf{NAME} & \textbf{STATUS} & \textbf{ROLES} & \textbf{AGE} & \textbf{VERSION} \\
\hline
nas-001       & Ready         & fast,once        & 82d & nfs \\
nas-002       & Ready         & fast,once        & 82d & nfs \\
nas-003       & Ready         & fast,once        & 82d & nfs \\
raspberry-009 & Ready         & slow,once        & 0d & nfs \\
raspberry-010 & NotReady         & slow,once        & 0d & nfs \\
raspberry-011 & NotReady         & fast,many        & 0d & csi \\
raspberry-012 & NotReady         & fast,many        & 0d & csi \\
raspberry-013 & NotReady         & fast,many        & 0d & csi \\
\hline
\end{tabular}
\caption{Estado de los Nodos del Clúster.}
\end{table}

\section{TLS y Certificados}
La gestión de certificados TLS para los servicios expuestos a través de Ingress está automatizada mediante \textbf{Cert-Manager}.
\begin{itemize}
    \item \textbf{Cert-Manager:} Es un controlador de Kubernetes que solicita y renueva automáticamente certificados TLS de diversas fuentes.
    \item \textbf{Let's Encrypt:} Se utiliza como la Autoridad de Certificación (CA) para emitir certificados gratuitos y confiables para los dominios públicos. Cert-Manager se comunica con Let's Encrypt para validar la propiedad del dominio y obtener los certificados para las reglas de Ingress que los soliciten.
\end{itemize}

La automatización de certificados TLS es posible gracias a la correcta configuración del flujo de red, desde el DNS hasta el Ingress Controller. El siguiente diagrama ilustra cómo Cert-Manager aprovecha este flujo para validar la propiedad del dominio y obtener un certificado válido de Let's Encrypt.

\begin{figure}[H]
\centering
\begin{tikzpicture}[node distance=1.5cm, auto, align=center]
    % Main Components (Vertical Flow)
    \node[component] (dns) {DNS\\ \small(app.com) \\\small161.132.4.98};
    \node[server, below=0.5cm of dns] (vps) {VPS Público \\ \small(161.132.4.98)};
    \node[component, below=0.5cm of vps] (frp_tunnel) {Túnel FRP \\ \small(frps \texttt{->} frpc)};
    \node[server, below=0.5cm of frp_tunnel] (ingress) {Nginx Ingress \\ \small(192.168.100.240)};
    \node[server, below=1cm of ingress] (pod) {Pod de la App};

    % Cert-Manager Components (to the side)
    \node[component, right=2cm of ingress] (cert_manager) {Cert-Manager};
    \node[component, above=of cert_manager] (lets_encrypt) {Let's Encrypt};
    \node[server, below=of cert_manager] (secret) {Kubernetes Secret \\ \small(Certificado TLS)};

    % Main Traffic Flow
    \draw[flow] (dns) -- (vps);
    \draw[flow] (vps) -- (frp_tunnel);
    \draw[flow] (frp_tunnel) -- (ingress);
    \draw[flow] (ingress) -- (pod);

    % Cert-Manager Flow
    \draw[flow, dashed] (ingress) -- node[above] {\small 1. Observa} (cert_manager);
    \draw[flow, dashed] (cert_manager) -- node[left] {\small2. Solicita} node[right] {4. Emite} (lets_encrypt);
    \draw[flow, dashed] (lets_encrypt.north) to node[above, sloped] {\small3. Challenge HTTP-01} (vps.east);
    \draw[flow, dashed] (cert_manager) -- node[left] {\small5. Guarda} (secret);
    \draw[flow, dashed] (secret) to node[below] {\small6. Usa Cert} (ingress);

\end{tikzpicture}
\caption{Flujo de Obtención de Certificados con Cert-Manager.}
\label{fig:cert_manager_flow}
\end{figure}

\end{document}